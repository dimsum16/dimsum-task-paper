\PassOptionsToPackage{usenames}{color}
\documentclass[11pt,letterpaper]{article}
\usepackage{etex} % remove ``No room for a new \dimen'' error

\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{style/naaclhlt2016}

\naaclfinalcopy % Uncomment this line for the final submission
%\def\naaclpaperid{***} %  Enter the naacl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\usepackage[colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=black]{hyperref}
\usepackage{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}

\usepackage{framed}
\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}
%%reduce space around captions
%\setlength{\abovecaptionskip}{1pt plus 1pt minus 1pt}
%\setlength{\belowcaptionskip}{1pt plus 1pt minus 1pt}

%\titlespacing*\section{0pt}{6pt plus 4pt minus 2pt}{4pt plus 2pt minus 2pt}
%\titlespacing*\subsection{1pt}{6pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
%\titlespacing*\subsubsection{1pt}{6pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
%\titlespacing*\paragraph{1pt}{6pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage{listings}

\lstset{
  basicstyle=\itshape,
  xleftmargin=3em,
  aboveskip=0pt,
  belowskip=-3pt, %-.5\baselineskip, % correct for extra paragraph break inserted after listing
  literate={->}{$\rightarrow$}{2}
           {α}{$\alpha$}{1}
           {δ}{$\delta$}{1}
           {(}{$($}{1}
           {)}{$)$}{1}
           {[}{$[$}{1}
           {]}{$]$}{1}
           {|}{$|$}{1}
           {+}{\ensuremath{^+}}{1}
           {*}{\ensuremath{^*}}{1}
}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled=.8]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}

% Graphics
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,calc} 



% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}
\newcolumntype{H}{>{\hspace{-15pt}\setbox0=\hbox\bgroup}c<{\egroup}@{}} % hidden column

\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
%\usepackage{tikz}
%\usepackage{tree-dvips}
%\usetikzlibrary{arrows,positioning,calc} 
\usepackage{xytree}

\usepackage{xspace} % \xspace command for macros (inserts a space unless followed by punctuation)

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\setlength\titlebox{4cm}    % Expanding the titlebox



% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
% \lstset{
%   language=Python,
%   upquote=true,
%   showstringspaces=false,
%   formfeed=\newpage,
%   tabsize=1,
%   commentstyle=\itshape\color{lavender},
%   basicstyle=\small\smaller\ttfamily,
%   morekeywords={lambda},
%   emph={upward,downward,tc},
%   emphstyle=\underbar,
%   aboveskip=0cm,
%   belowskip=-.5cm
% }
%\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\nasmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{NA}}_{\textsc{S}}}}}}
\newcommand{\dhmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{D}}_{\textsc{H}}}}}}
\newcommand{\ajmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{A}}_{\textsc{J}}}}}}
\newcommand{\lkmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{L}}_{\textsc{K}}}}}}
\newcommand{\swswmarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{S}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\mcmarker}{\ensuretext{\textcolor{dkblue}{\ensuremath{^{\textsc{M}}_{\textsc{C}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\aj}[1]{\arkcomment{\ajmarker}{#1}{purple}}
\newcommand{\dirk}[1]{\arkcomment{\dhmarker}{#1}{red}}
\newcommand{\lk}[1]{\arkcomment{\lkmarker}{#1}{blue}}
\newcommand{\swsw}[1]{\arkcomment{\swswmarker}{#1}{orange}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{purple}}
\newcommand{\mc}[1]{\arkcomment{\mcmarker}{#1}{dkblue}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
%\crefmultiformat{part}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
%\crefmultiformat{chapter}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{section}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsubsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
%\crefrangeformat{part}{\mbox{\S\S#3#1#4--#5#2#6}}
%\crefrangeformat{chapter}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{section}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsubsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{paragraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefrangeformat{subparagraph}{\mbox{\P\P#3#1#4--#5#2#6}}
% for \label[appsec]{...}
\crefname{part}{Part}{Parts}
\Crefname{part}{Part}{Parts}
\crefname{chapter}{ch.}{ch.}
\Crefname{chapter}{Ch.}{Ch.}
\crefname{figure}{figure}{figures}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{algocf}{algorithm}{algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}
\crefname{enums,enumsi}{example}{examples}
\Crefname{enums,enumsi}{Example}{Examples}
\crefname{}{example}{examples} % lingmacros \toplabel has no internal name for the kind of label
\Crefname{}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}
\crefformat{enumsi}{(#2#1#3)}
\crefformat{}{(#2#1#3)}
\crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}

\ifx\creflastconjunction\undefined%
\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\else%
\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\fi%

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{footnote~\ref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes
\newcommand{\Fnref}[1]{Footnote~\ref{#1}}

\widowpenalty = 10000 
\displaywidowpenalty = 10000 
\clubpenalty = 10000


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
% \addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
% \addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
% \addtolength{\abovedisplayskip}{-.5cm} % space before maths
% \addtolength{\belowdisplayskip}{-.5cm} % space after maths
% %\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
\setlength{\belowcaptionskip}{-.5cm}


% customize \paragraph spacing
% \makeatletter
% \renewcommand{\paragraph}{%
%   \@startsection{paragraph}{4}%
%   {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
%   {\normalfont\normalsize\bfseries}%
% }
% \makeatother


% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% tag name
\newcommand{\sst}[1]{\textsc{#1}} % supersense category
\newcommand{\gfl}[1]{%\renewcommand\texttildelow{{\lower.74ex\hbox{\texttt{\char`\~}}}} % http://latex.knobs-dials.com/
\mbox{\textsmaller{\texttt{#1}}}}	% supersense tag symbol
%\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\tagdef}[1]{#1\hfill} % tag definition
\newcommand{\tagt}[2]{\ensuremath{\underset{\textrm{\textlarger{\tg{#2}}}\strut}{\w{#1}\rule[-.3\baselineskip]{0pt}{0pt}}}} % tag text (a word or phrase) with an SST. (second arg is the tag)
\newcommand{\tagtt}[3]{\begin{tabular}{@{\hspace{2pt}}c@{\hspace{2pt}}} \texttt{#2}\\ #1 \\ \texttt{#3}\end{tabular}}
\newcommand{\tagts}[3]{\begin{tabular}{@{\hspace{2pt}}c@{\hspace{2pt}}} \texttt{#2\vphantom{\textlarger{Ĩ}}}\\ #1 \\ \sst{#3}\end{tabular}}
\newcommand{\tagtss}[5]{\begin{tabular}{@{\hspace{2pt}}c@{\hspace{2pt}}} \texttt{#2\vphantom{\textlarger{Ĩ}}}\\ \sst{#3}\vphantom{X} \\ \textbf{#1}\vphantom{lp} \\ \sst{#5}\vphantom{X} \\ \texttt{#4\vphantom{\textlarger{Ĩ}}}\end{tabular}}
\newcommand{\tgt}[2]{\ensuremath{\underset{\text{\textlarger{#2}\strut}}{\text{#1\rule[-.3\baselineskip]{0pt}{\baselineskip}}}}} % tag text (a word or phrase, not underlined) with a label. (second arg is the label)
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\textrm{#1}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\AnnA}[0]{\mbox{\textbf{Ann-A}}} % annotator A
\newcommand{\AnnB}[0]{\mbox{\textbf{Ann-B}}} % annotator B
\newcommand{\sys}[1]{\mbox{\texttt{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\lex}[1]{\textit{#1}} % lexical item
\newcommand{\tweet}[1]{\textsf{#1}}	% tweet
\newcommand{\twbank}[0]{\textsc{Tweebank}\xspace}
\newcommand{\foster}[0]{\textsc{Foster}\xspace}
\newcommand{\twparser}[0]{\textsc{Tweeboparser}\xspace}
\newcommand{\tat}[0]{\textasciitilde}
\newcommand{\backtick}[0]{\textasciigrave}

%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\shortversion}[1]{}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{#1} % ...if only there were more space...

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{SemCor}
\hyphenation{SemEval}
\hyphenation{PennConverter}
\hyphenation{TurboParser}
\hyphenation{Tweebo-parser}
\hyphenation{AMALGrAM}
\hyphenation{Twee-bank}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}


\title{SemEval-2016 Task~10:\\ Detecting Minimal Semantic Units and their Meanings (DiMSUM)}

\author{
Nathan Schneider \\
		School of Informatics\\
	   	University of Edinburgh\\
	    Edinburgh, UK\\ % email won't fit here, so put them all in the middle column, comma-separated
	\And
Dirk Hovy \quad Anders Johannsen\\
Center for Language Technology\\
University of Copenhagen\\
Copenhagen, Denmark\\
{\tt nschneid@inf.ed.ac.uk}, {\tt krx628@hum.ku.dk}, {\tt anders@johannsen.com}, {\tt marine@cs.umd.edu} 
    \And
Marine Carpuat\\
Computer Science Dept.\\
University of Maryland\\
College Park, Maryland, USA\\ % email won't fit here, so put them all in the middle column, comma-separated
}


\date{}

\begin{document}
% \nss{Note: this document uses natbib rather than the standard ACL bib macros: 
% According to \citet{baldwin-10}, statistical association measures 
% are one technique for extracting MWE types \citep[cf.][\emph{inter alia}]{pecina-10}.}
\naaclfinalcopy % show authors
\maketitle
%\global\naaclfinalfalse % turn on header/line numbers for review

\begin{abstract}
This task combines the labeling of multiword expressions and supersenses (coarse-grained classes) in an explicit, yet broad-coverage paradigm for lexical semantics.
Nine systems participated; the best scored 57.7\% $F_1$ in a multi-domain evaluation setting, 
indicating that the task remains largely unresolved. 
An error analysis reveals that a large number of instances in the data set are either hard cases, 
which no systems get right, or easy cases, which all systems correctly solve.
%We propose a SemEval~2016 Shared Task 
%of analyzing the lexical semantics of English sentences 
%in a broad-coverage fashion. 
%The task formulation consists of two closely related steps which can be performed jointly:
%(a)~chunking tokens within sentences into \textbf{multiword expressions}, and 
%(b)~assigning coarse \textbf{supersense} labels to all noun and verb expressions. 
%The evaluation will build upon recently annotated datasets in two social web genres.
%\longversion{We expect that the task formulation will foster engagement across 
%multiple subcommunities of computational semantics, 
%and facilitate empirical comparison of methodologically diverse systems.}
\end{abstract}

\section{Introduction}\label{sec:intro}


% Broad-coverage NLP techniques such as XXX\dirk{Nathan, could you make clear what broad coverage is?} have
% become mainstream for several aspects of language---including morphology, syntax, 
% and some areas of semantics (notably, named entity recognition, %\nss{2002/2003}
% semantic role labeling, %\nss{2004/2005/2008/2009} 
% and coreference resolution). %\nss{2011/2012}
% %, thanks in part to previous SemEval shared tasks,

Grammatical analysis tasks, e.g., part-of-speech tagging, are rather successful applications of natural language processing (NLP).  They are \emph{comprehensive}, i.e., they 
operate under the assumption that all grammatically-relevant parts of a sentence  
will be analyzed: We do not expect a POS tagger to only know a subset of the tags in the language. 
Most POS tags accommodate unseen words and adapt readily to new text genres.
Together, these factors indicate a representation which achieves \emph{broad coverage}. 

Explicit analysis of lexical semantics, by contrast, has been more difficult to scale 
to broad coverage owing to limited comprehensiveness and extensibility.
The dominant paradigm of fine-grained word sense disambiguation, WordNet \citep{wordnet}, 
is difficult to annotate in corpora, results in considerable data sparseness, 
and does not readily generalize to out-of-vocabulary words.
While the main corpus with WordNet senses, 
SemCor \citep{semcor}, does reflect several text genres, it is hard
to expand SemCor-style annotations to new genres, such as social web text 
or transcribed speech.
This severely limits the applicability of SemCor-based NLP tools 
and restricts opportunities for linguistic studies of lexical semantics in corpora.

\finalversion{\nss{do we want to say something about insights from MT? this needs work:}
Consequently,  \citet{carpuat-05,resnik-06} found that traditional, fine-grained word sense disambiguation 
did indeed \emph{not} improve machine translation. 
\nss{TODO}However, abstracting the senses and moving to multi-word does \citep{chan-07,carpuat-07,lefever-10}.
}

%The goal of this task is therefore to establish a broad-coverage paradigm for lexical semantic analysis that works on a variety of genres.
%translations in a second language replaced abstract word senses, 
%and the single-word disambiguation targets were replaced by the multiword phrases used 
%by translation systems 

To address this limitation, in the DiMSUM~2016 shared task,\footnote{\url{http://dimsum16.github.io/}} we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating
\textbf{multiword expressions} and \textbf{noun and verb supersenses} \citep[following][]{schneider-15}, 
on multiple nontraditional genres of text. 
By moving away from fine-grained sense inventories and lexicalized, language-specific\footnote{Though our data set is limited to English, the representation is applicable to other languages: see \cref{sec:integration}.} annotation,
we take a step in the direction of broad-coverage, coarse-grained lexical semantic analysis. 
We believe this departure from the classical lexical semantics paradigm 
will ultimately prove fruitful for a variety of NLP applications in a variety of genres. 
%to establish a broad-coverage paradigm for lexical semantic analysis that works on a variety of genres.

% \nss{motivation: lexical semantic analysis of MWEs and supersenses in English. 
% semantics is a hot topic.
% highlights: (a) broad-coverage---not too far from NER; hopefully avoid some of the limitations of traditional WSD. 
% (b) robust to domain---in fact, evaluation consists of 2 social web domains.
% (c) potential to benefit from many different NLP techniques, including sequence tagging/chunking, 
% parsing, distributional word representations, language models, 
% use of language resources such as WordNet, (other buzzwords?)}

%\paragraph{Summary}
%To date, many of the analysis problems in computational lexical semantics are limited in scope, due to granularity and coverage.
%We integrate broad-coverage MWE identification with supersense tagging 
The integrated lexical semantic representation (\cref{sec:integration}, \cref{sec:rep})
has been annotated in an extensive benchmark data set comprising several nontraditional domains (\cref{sec:data}).
Objective, controlled evaluation procedures (\cref{sec:eval})
facilitate a comparison of the 9~systems submitted as part of the official task (\cref{sec:results}).
While the systems range in performance, all are below 60\% in our composite evaluation, 
suggesting that further work is needed to make progress on this difficult task.



% \nss{encourage creativity of solutions}
% Semantic analysis is a powerful and promising topic, but it has been largely confined to news text, 
% i.e., standardized text written mostly in English. 
% By including a relatively new and heterogeneous domain like Twitter in the data, 
% we want to encourage creative approaches that go beyond the tried and true newswire-trained approach, 
% and open the community to wider applications. 

% This task would capitalize on current enthusiasm for semantics in NLP, 
% providing a testbed for a wide variety of technical approaches---these could include 
% linguistic feature engineering; comparing sequence models to hierarchical parsing models, or their combination 
% with joint decoding \citep{le_roux-14}; domain adaptation \citep{johannsen-14}; 
% type supervision; word clustering\slash representation learning \citep{grave-13}; 
% collocation extraction \citep{pecina-10,ramisch-12}; 
% language modeling; or other methods that exploit additional resources.

% This is a new task. 
% We believe it will attract strong participation from researchers in lexical semantics, 
% structured prediction, (semi)supervised learning, domain adaptation, and social media NLP. 
% To gauge interest, we reached out informally to a number of leaders in the field, 
% asking for feedback on the idea. Several of them have indicated that they consider this 
% a worthwhile task and will consider participating if it is accepted.
% As a rough estimate, we think it would attract participation from about 20~teams.
% If it is accepted, we will publicize the task widely in order to recruit as much participation as possible.


\section{Background}\label{sec:integration}
\textbf{Multiword expressions.} 
Most contemporary approaches to English syntactic and semantic analysis
treat space-separated words as the basic units of structure. 
%It has even been suggested that dependency syntax, for example, 
%offers a good approximation of semantic structure.
However, this fails to reflect 
the basic units of meaning for sentences with non-compositional or idiosyncratic expressions, such as:
%\begin{enumerate}[leftmargin=*,labelsep=2em,label=(\arabic*),before=\raggedright]
\begin{itemize}[labelindent=2em]
\item[(1)] The staff {\color{red}leaves} {\color{blue}a lot} {\color{red}to be desired} .
\item[(2)] \raggedright I googled restaurants in the area and {\color{red}Fuji Sushi} {\color{blue}came up} and reviews were great so I {\color{mdgreen}made a} {\color{orange}carry out} {\color{mdgreen}order} of : L 17 .
\end{itemize}
In these sentences, \lex{a lot}, \lex{leaves\ldots to be desired}, 
\lex{Fuji Sushi}, \lex{came up}, \lex{made\ldots order}, and \lex{carry out}
are all \textbf{multiword expressions} (MWEs): their combined meanings can be thought of as ``prepackaged'' 
in a single lexical expression that happens to be written with spaces.
%\footnote{This is not to say that MWEs cannot have ``normal'' 
%syntactic structure; some are more fixed in their form while others are quite flexible. 
%For instance, \lex{make\ldots order} can be passivized just like canonical transitive verb phrases.}
MWEs such as these have attracted a great deal of attention 
within computational semantics; see \citet{baldwin-10} for a review.
\Citet{schneider-14-corpus} introduced an English corpus resource annotated for heterogenous MWEs, 
suitable for training and evaluating general-purpose MWE identification systems \citep{schneider-14}.
Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds \citep[recently:][]{constant-11,green-12,ramisch-12,vincze-13},
though the corpus and identification system of \citet{vincze-11} targets several kinds of MWEs.

Importantly, the MWEs in \citeposs{schneider-14-corpus} corpus are not required to be contiguous, 
but may contain \textbf{gaps} (viz.: \lex{made\ldots order}). The corpus also contains qualitative labels indicating the strength of MWEs, either \textbf{strong} (mostly non-compositional) or \textbf{weak} (compositional but idiomatic). For simplicity we only include strong MWEs in this task.

\textbf{Supersenses.}
% \longversion{
% Ideally, a lexical semantic representation would apply to a large number of word types 
% while being sufficiently fine-grained to disambiguate words with multiple meanings. 
% %A representation represents good coverage if it provides labels for most word classes. 
% %It offers sufficient distinction if it allows one to group words into clusters, 
% %in order to make generalizations about semantic classes. 
% A good meaning representation should also group together related meanings from different words. 
% %is typically anti-correlated with the number of available labels.
% These ideals are sometimes at odds:  \nss{\ldots}
% 
% Popular representations are named-entity tags and WordNet \citep{wordnet} word senses. 
% However, both present a problem in one of the desired aspects. 
% Named entity tags have a reasonable distinction, and are thus frequently used to find entities of a certain type. 
% However, as the name already suggests, this representation is limited to (proper) nouns, and even though high-coverage gazetteers can be found for named entity recognition, the task has thus inherently poor coverage. 
% Word senses, on the other hand, cover all content word classes. 
% However, they are lemma-specific (i.e., sense number two of \emph{house} and \emph{building} are not the same), 
% and do not allow for generalization to larger groups.\nss{not true of WordNet, because the hierarchy allows generalization. 
% I think the problems with the WordNet-style approach are (a)~it is hard to agree on/annotate with fine-grained senses, 
% and (b)~it is hard to extend the lexicon to cover OOV words that might occur in new domains (let alone other languages).}
% }
As noted above, relying on WordNet-like fine-grained, lexicalized \textbf{senses} 
creates problems for annotating at a large scale and covering new domains and languages. 
Named entity recognition (NER) does not suffer from these problems, 
as it uses a much smaller number of coarse-grained classes. 
However, these classes only apply to a subset of the nouns in a sentence and exclude verbs and adjectives.
They therefore provide far from complete coverage in a corpus.

Noun and verb \textbf{supersenses} \citep{ciaramita-06}
offer a middle ground in granularity: they generalize 
named entity classes to cover all nouns (with 26~classes), but also cover verbs (15~classes)---see \cref{tbl:supersenses}---and provide a human-interpretable high-level clustering. WordNet supersenses for adjectives and adverbs nominally exist, 
but are based on morphosyntactic rather than semantic properties.  
There is, however, recent work on developing supersense taxonomies for English adjectives and prepositions \citep{tsvetkov-14,schneider-15-law}.

The inventory for nouns and verbs originates from the top-level organization of WordNet,
but can be applied directly to annotate new data---including out-of-vocabulary words 
in English or other languages \citep{schneider-12,johannsen-14}. 
Similar to NER, supersense tagging approaches have generally used statistical sequence models 
and have been evaluated in English, Italian, Chinese, Arabic, and Danish.\footnote{Evaluations used 
English SemCor \citep{ciaramita-06,paas-09}, 
English-Italian MultiSemCor \citep{picca-08,picca-09,attardi-10}, 
the Italian Syntactic-Semantic Treebank and Italian Wikipedia \citep{attardi-10,rossi-13}, 
Chinese Cilin \citep{qiu-11},  
Arabic Wikipedia \citep{schneider-13}, and the Danish CLARIN Reference Corpus \citep{martinez-15}.}

Features based on supersenses have been exploited in downstream semantics tasks\longversion{ 
such as preposition sense disambiguation, noun compound interpretation, 
question generation, and metaphor detection} 
\citep{ye-07,hovy2010,tratz-10,heilman-thesis,hovy-2013identifying,tsvetkov-13}. 

\begin{table}\centering\small
\resizebox{ \columnwidth }{!}{
\begin{tabular}{>{\sst\bgroup}l<{\egroup}>{\sst\bgroup}l<{\egroup}>{\sst\bgroup}l<{\egroup}}
n:Tops	&	n:object	&	v:cognition\\
n:act	&	n:person	&	v:communication\\
n:animal	&	n:phenomenon	&	v:competition\\
n:artifact	&	n:plant	&	v:consumption\\
n:attribute	&	n:possession	&	v:contact\\
n:body	&	n:process	&	v:creation\\
n:cognition	&	n:quantity	&	v:emotion\\
n:communication	&	n:relation	&	v:motion\\
n:event	&	n:shape	&	v:perception\\
n:feeling	&	n:state	&	v:possession\\
n:food	&	n:substance	&	v:social\\
n:group	&	n:time	&	v:stative\\
\cline{2-2}
n:location	&	v:body	&	v:weather\\
n:motive	&	v:change	&	
\end{tabular}
}% resize
\caption{The 41 noun and verb supersenses in WordNet.}
\label{tbl:supersenses}
\end{table}

% \longversion{
% The \textbf{supersense} categories for nouns and verbs, \cref{tbl:supersenses},
% are also derived from WordNet, but represent a higher level in the taxonomy. 
% They originated from a higher-level clustering of nouns and verbs into groups, 
% and thus have good coverage.\footnote{Supersenses for adjectives and adverbs nominally exist, 
% but are based on morphosyntactic rather than semantic properties. 
% There is, however, recent work on developing a taxonomy for English adjective supersenses \citep{tsvetkov-14}.}
% Since they group individual word senses into larger groups, they allow for distinction beyond the lemma level, 
% i.e., they assign the same supersense to words with different word senses. Supersenses thus provide 
% human-interpretable high-level clustering information, and have mostly been used as features in other tasks, 
% such as preposition sense disambiguation, noun compound interpretation, and metaphor detection 
% \cite{Ye:Baldwin:2007,Tratz:Hovy:2010,Tsvetkov:ea:2013}. 
% 
% The task of supersense tagging was first introduced by \citep{Ciaramita:Altun:2006EMNLP}, 
% who used 5-fold cross validation on {\sc SemCor} to evaluate their model (structured 1st order discriminative). 
% Their evaluation also included a held-out development set on each fold that was used to estimate the number of epochs 
% and additional training data containing only verbs. Their overall $F_1$ score on {\sc SemCor} was 77.1.
% Reichartz and Paa\ss{} \citep{Reichartz:Paass:2008ECMLPKDD,Paass:Reichartz:2009SIAM} 
% reached an $F_1$ score of 80.2, using CRFs and additional features.
% 
% There has been relatively little recent work on supersense tagging, and to the best of our knowledge, all of it has been mostly limited to standard English\footnote{With few exceptions such as \cite{schneider-12,schneider-13}.} ({\sc SemCor} and {\sc SensEval}). We are not aware of any previous work on word sense disambiguation for Twitter. Our hope is that this task makes the problem more attractive to a wider audience to reach more languages and domains.
% }


\textbf{Relationship between MWEs and supersenses.}
We believe that MWEs and supersenses should be tightly coupled:
idiomatic combinations such as MWEs are best labeled holistically,
since their joint supersense category will often differ from that of the individual words.
For example, \lex{spill the beans} in its literal interpretation
would receive supersenses \sst{v:contact} and \sst{n:food},
whereas the idiomatic interpretation, `divulge a secret', is represented as
an MWE holistically tagged as \sst{v:communication}.
\Citet{schneider-15} develop this idea at length, and provide a web reviews data set
with the integrated annotation. Here, we expand the paradigm to additional domains
and compare the performance of several systems.


\section{Representation}\label{sec:rep}

\begin{figure}\centering\small
%I highly~recommend_ Garage_Pros _to my friends .  
%"labels": {"8": ["friends", "GROUP"], "3": ["recommend", "communication"], "4": ["Garage", "GROUP"]}, "_": [[3, 6], [4, 5]], "words": [["I", "PRP"], ["highly", "RB"], ["recommend", "VBP"], ["Garage", "NNP"], ["Pros", "NNPS"], ["to", "IN"], ["my", "PRP$"], ["friends", "NNS"], [".", "."]], "~": [[2, 3, 6]]}
\begin{tikzpicture}[baseline={($(current bounding box.center)+(0,-0.5ex)$)}, node distance=0cm, auto,]
 \node[inner sep=0cm]                                  (n1) {\tagts{The}{O}{}};
 \node[inner sep=0cm,above right=0pt of n1.south east] (n2) {\tagts{staff}{O}{\textsc{n.group}}};
 \node[inner sep=0cm,above right=0pt of n2.south east] (n3) {\tagts{leaves}{\color{red}B}{\color{red}\textsc{v.cognition}}};
 \node[inner sep=0cm,above right=0pt of n3.south east] (n4) {\tagts{a}{\color{blue}b}{}};
 \node[inner sep=0cm,above right=0pt of n4.south east] (n5) {\tagts{lot}{\color{blue}i}{}};
 \node[inner sep=0cm,above right=0pt of n5.south east] (n6) {\tagts{to}{\color{red}I}{}};
%\path[-,thick,dotted, bend left=30] (n2.north) edge (n3.north); 
\path[-,thick,red, bend left=20] (n3.north) edge (n6.north);
 \path[-,thick,blue, bend left=30] (n4.north) edge (n5.north);
 \node[inner sep=0cm,above right=0pt of n6.south east] (n7) {\tagts{be}{\color{red}I}{}};
 \node[inner sep=0cm,above right=0pt of n7.south east] (n8) {\tagts{desired}{\color{red}I}{}};
\path[-,thick,red, bend left=30] (n6.north) edge (n7.north);
\path[-,thick,red, bend left=30] (n7.north) edge (n8.north);
 \node[inner sep=0cm,above right=0pt of n8.south east] (n9) {\tagts{.}{O}{}};
\end{tikzpicture}
\caption{Illustration of the target representation. MWE positional markers are shown above the sentence 
and noun and verb supersenses below the sentence. 
Links illustrate the behavior of the MWE tags.
The supersense labeling must respect the MWEs; thus, \sst{v.cognition} 
applies to a four-word unit---\emph{to}, \emph{be}, and \emph{desired} 
must not receive separate supersenses from \textit{leaves}.}
\label{fig:ex1}
\end{figure}

% ewtb.r.055976.1 The staff leaves_ a_lot _to_be_desired .
% ewtb.r.389298.4 I highly~recommend_ Garage_Pros _to my friends .  
%"labels": {"8": ["friends", "GROUP"], "3": ["recommend", "communication"], "4": ["Garage", "GROUP"]}, "_": [[3, 6], [4, 5]], "words": [["I", "PRP"], ["highly", "RB"], ["recommend", "VBP"], ["Garage", "NNP"], ["Pros", "NNPS"], ["to", "IN"], ["my", "PRP$"], ["friends", "NNS"], [".", "."]], "~": [[2, 3, 6]]}
% ewtb.r.329991.1 I googled restaurants in the area and Fuji_Sushi came_up and reviews were great so I made_ a carry_out _order of : L 17 .       {"labels": {"2": ["googled", "communication"], "3": ["restaurants", "GROUP"], "6": ["area", "LOCATION"], "8": ["Fuji", "GROUP"], "10": ["came", "communication"], "13": ["reviews", "COMMUNICATION"], "14": ["were", "stative"], "18": ["made", "communication"], "20": ["carry", "possession"]}, "_": [[8, 9], [10, 11], [18, 22], [20, 21]], "words": [["I", "PRP"], ["googled", "VBD"], ["restaurants", "NNS"], ["in", "IN"], ["the", "DT"], ["area", "NN"], ["and", "CC"], ["Fuji", "NNP"], ["Sushi", "NNP"], ["came", "VBD"], ["up", "RB"], ["and", "CC"], ["reviews", "NNS"], ["were", "VBD"], ["great", "JJ"], ["so", "RB"], ["I", "PRP"], ["made", "VBD"], ["a", "DT"], ["carry", "VB"], ["out", "RP"], ["order", "NN"], ["of", "IN"], [":", ":"], ["L", "$"], ["17", "CD"], [".", "."]], "~": []}


The analysis for each sentence is represented as 
a sequence of paired MWE and supersense tags. \Cref{fig:ex1} illustrates 
the MWE part above the sentence and the supersense part below the sentence.

The MWE portion is a BIO-style \citep{ramshaw-95} 
positional marker. Of the schemes discussed by \citet{schneider-14}, 
we adopt the 6-tag scheme, which uses case to allow gaps in an MWE (lowercase tag variants mark 
tokens within a gap).
The positions are thus \texttt{O}, \texttt{o}, \texttt{B}, \texttt{b},
\texttt{I}, \texttt{i}.
Systems are expected to ensure that the full tag sequence for a sentence is valid: 
global validity can be enforced with first-order constraints 
to prohibit invalid bigrams such as \texttt{O~I} and \texttt{b~I} (see \citealp{schneider-14} for details).

Because strong MWEs receive a supersense as a unit (if at all), 
\texttt{I} and \texttt{i} are never accompanied by a supersense label.
\texttt{O} or \texttt{o} indicates that the token is not part of any MWE, 
but many such tokens do bear a noun or verb supersense.
%\longversion{Of course, the supersenses in this task only cover noun and verb expressions, 
%so plain \texttt{O} will be frequent, as will plain \texttt{B} and \texttt{Ī} for (e.g.) PP idioms.}

This task uses a CoNLL-style main \textbf{file format}
consisting of one line per token, each line having 9~tab-delimited columns.
Scripts to convert to and from the \texttt{.sst} format, which displays one sentence per line 
and contains annotations in a JSON data structure, are provided as well.

\section{Data}\label{sec:data}

The task built upon two existing data sets of social web text, which were 
harmonized to form the training data. Four new samples from three domains 
were newly annotated to form the test set.
The train and test sets are summarized in \cref{tbl:srcdatasets,tbl:annotateddata} 
and are publicly available on the web.\footnote{\url{https://github.com/dimsum16/dimsum-data}}

\begin{table*}\small\centering
\input{sourcedatasets.tex}
\label{tbl:srcdatasets}
\end{table*}

\begin{table*}\small\centering
\input{annotateddatasets.tex}
\label{tbl:annotateddata}
\end{table*}

The domains covered are \textbf{online customer reviews}, 
\textbf{tweets}, and \textbf{TED talks}.
This section describes, for each domain, how its component data sets were sampled, preprocessed, 
and annotated.



\subsection{Annotation Process}
We compiled data sets from various sources, with varying degrees of existing pre-annotation. 
Unless already provided, we added Universal POS tags as defined by the Universal Dependencies project \citep{nivre-15}, 
and baseline supersenses (heuristically using the most frequent WordNet sense, and
in some cases grouping sequences of proper nouns as MWEs). % apparently PROPN -> MWE heuristics were used for TED but not Trustpilot? Hence ``in some cases'' 
%(MWEs annotated in the gold Tweebank parses were retained.) % this is noted in the Tweebank section below
The pre-annotated supersenses were then manually corrected by a trained annotator, 
who simultaneously annotated the sentence for comprehensive MWEs. 

The annotator (a linguist) was trained by the first author of this paper  
using \citeposs{schneider-15} web interface and annotation guidelines.
Prior to starting on the data sets for this task,
the annotator devoted approximately 8~hours to training practice 
on a separate data set which already had a gold standard.
Periodic feedback was given on initial annotations as the annotator grew accustomed to the conventions.
The annotator spent approximately 50~hours on DiMSUM data (not including the initial training phase), 
which amounts to roughly 90 seconds per sentence. 

In order to estimate inter-annotator agreement (IAA),
the first author independently annotated a sample of Ritter tweets (\cref{sec:tweets}) 
in 6~groups of 11~sentences, spaced out across the main annotator's annotation batches. 
IAA estimates for these sets ranged from 60\% to 75\% $F_1$ for MWEs, and 
67\%--80\% accuracy for supersenses (on tokens which had supersenses in both annotations).
% https://docs.google.com/spreadsheets/d/13IRaz-K98Css-72Ft4u5FtEYDbCHay8qnRXmD-oEEDU/edit#gid=0
Resources did not allow for more systematic double annotation and IAA estimation 
throughout the data.

% $ cd /Users/nathan/dev/e/2016DiMSUM_data
% $ diff --side-by-side --suppress-common-lines <(conversion/test/sst2dimsum.py conversion/test/test.shuf.sst) dimsum16.test | wc -l
%     3120
% $ sed '/^$/d' dimsum16.test | wc -l
%    16500
%% 3120/16500 = 19% of tokens were affected by a change to MWE boundaries and/or supersenses.

The test set newly annotated for this task comprises exactly 1,000 sentences and exactly 16,500 words.
3,120~word tokens (19\%) differ from the pre-annotation with respect to gold MWE boundaries and/or supersenses.\footnote{On the surface, 
this might be taken to mean that the accuracy of the heuristic baseline used for pre-annotation is 81\%. 
However, because the annotator saw the pre-annotation, we expect that this agreement rate 
is higher than if the gold standard had been produced from scratch.}
In addition, portions of the training data were reannotated for improved quality and consistency 
with the STREUSLE annotations, as explained below.

\subsection{\dataset{Reviews}} 
\paragraph{Training.}
The \dataset{Reviews} part of the training data consists of the STREUSLE corpus \citep{schneider-14-corpus,schneider-15},\footnote{\url{http://www.ark.cs.cmu.edu/LexSem/}}
comprising comprehensive multiword expression and supersense annotations on a 55,000-token portion of the English Web Treebank \citep[EWTB;][]{ewtb} 
made up of 723~online user reviews for services (such as restaurants and beauticians).

STREUSLE annotation was done by linguists, who took pains to establish conventions and resolve disagreements.
Each sentence was annotated independently by at least 2~annotators; disagreements were resolved by negotiation. 
%Annotators found approximately 3,500~MWEs, 9,000~noun expressions, and 8,000~main verb expressions (excluding auxiliary verbs).\finalversion{\nss{
%(\Cref{tbl:reviews} offers a more detailed breakdown.)}}
%15\% of MWEs contain a gap.
%% (this may be confusing as the table shows slightly lower numbers due to filtering weak MWEs)

The task release is based on version~2.1 of STREUSLE, with weak MWEs removed
and Penn Treebank--style POS tags replaced with Universal POS tags.\footnote{The PTB-to-UPOS conversion script is available at:
\mbox{\url{http://tiny.cc/ptb2upos}}} 


\paragraph{Test.}
The test portion comprises 340 sentences (6,357 tokens) from the online review site Trustpilot, a subset of the data used in \citet{hovy-2015age} 
(the website as a general resource was described in \citet{hovy-2015trustpilot}). 
The reviews were chosen to obtain a demographic balance (by age, gender, and location), and contained gold POS tags. 

\subsection{\dataset{Tweets}}\label{sec:tweets} 
\paragraph{Training.}
\citet{johannsen-14} recently annotated two samples of 987 Twitter messages (18,000 words) 
with supersenses\longversion{: 
(a)~the POS+NER-annotated data set of \citet{ritter-11}, 
and 
(b)~\citeposs{plank-14} sample of 200~tweets}.\footnote{The supersense-annotated tweets 
are available at \url{https://github.com/coastalcph/supersense-data-twitter}.}
%, which we use for additional, out-of-sample evaluation. 
%We call these data sets {\sc Ritter}-$\{${\sc Train,Dev,Eval}$\}$ and {\sc In-House-Eval}, respectively. 
%The latter was downloaded in 2013 and was previously used (with part-of-speech annotation) in \citep{Plank:ea:14}.
%For both data sets, 
Annotators were shown pre-annotations from a heuristic supersense chunking\slash tagging system% 
\longversion{ (based on the most frequent sense of each word)}
and asked to correct the boundaries and supersense labels. 
%The supersenses are annotated with spans defined by the BIO (Begin-Inside-Outside) notation. 
Though there was no explicit MWE annotation phase, 
many of the multiword chunks tagged with a \longversion{noun or verb }supersense would be considered MWEs.\finalversion{\nss{POS?}}

We fully reannotated both data sets to match the conventions of the \dataset{Reviews} data from the STREUSLE corpus. 
The annotator examined every sentence and corrected any MWE or supersense decisions deemed to be inconsistent with the guidelines.

%\nss{commented out agreement numbers---I don't think they're necessary for this proposal, given that the data sets are already published}
% The average raw agreement on the data from Ritter was 0.86 (Cohen's $\kappa$ 0.77). 
% The majority of tokens received the \emph{O} label by all annotators; this happended in 515 out of 841 cases. 
% Excluding these instances to evaluate the performance on the more difficult content words, 
% raw agreement dropped to 0.69 and Cohen's $\kappa$ to 0.69.
% The second data had an average raw agreement of 0.65 and their Cohen's $\kappa$ 0.62. 
%For this data set, we also compute $F_1$, defined as usual as the harmonic mean of recall and precision. To compute this, we set one of the annotators as gold data and the other as predicted data. However, since $F_{1}$ is symmetrical, the order does not matter. The annotation $F_{1}$ gives us another estimate of annotation difficulty. We present the figures in Table \ref{table:properties-of-dataset}.
%For the task we plan to revise the boundaries in the data with greater attention to MWE criteria.
%This will be the only new annotation required, and we anticipate that it will be feasible to complete it 
%well before the July~30, 2015 release of the training data.
% (CPH should have resources to annotate a new Twitter test set with supersenses. possibly MWEs as well)
% (participants should be banned from using the CMWE data lest they train on the test set.)
% (is it OK if, e.g., supersense tagging conventions are different in different datasets?)
% be as specific as possible about timeline}

\paragraph{Test.}
Our test set consists of 500 tweets (6,627 tokens) taken from the Tweebank corpus \citep{kong-14},\footnote{\url{http://www.cs.cmu.edu/~ark/TweetNLP/}} 
which already contained some gold-standard MWEs. 
We converted the POS tags from gold ARK TweetNLP POS + FUDG dependencies to UPOS 
and had an annotator supply supersenses.


\subsection{\dataset{TED Talks}}
\paragraph{Test.}
To test the broad-coverage aspect of the submitted systems, the test set contained a ``surprise'' domain. 
We opted to sample transcribed sentences from TED talks. 
Because individual TED talks tend to heavily repeat vocabulary, we took the first 10 sentences 
from each of 16 documents in order to achieve a lexically diverse sample. 
Specifically, we chose 
(a)~100 sentences (2,187 tokens) from the 10 talks in the 
NAIST-NTT Ted Talk Treebank\footnote{\url{http://ahclab.naist.jp/resource/tedtreebank/}} \citep{neubig-2014naist} 
(which in turn is a subset of the IWSLT training data); 
and (b)~60 sentences (1,329 tokens) from the IWSLT test data \citep{cettolo-2012wit3}.\footnote{\url{https://wit3.fbk.eu/}}
The latter 6~documents were chosen to maximize language pair diversity.\footnote{These 6~talks
are known to have been translated from English into (at least) the following languages: 
\{ar, de, es, fa, he, hi, it, ko, nl, th, vi, zh\}. Additionally, we note that 4 of the documents have Czech (cs) 
translations, while the other 2 have French (fr) translations.

\Citet{neubig-2014naist} 
report that all the 10~documents in the NAIST-NTT Treebank have been translated from English into the following 
18~languages: \{ar, bg, de, el, es, fr, he, it, ja, ko, nl, pl, \mbox{pt-BR}, ro, ru, tr, \mbox{zh-CN}, \mbox{zh-TW}\}. 
Many additional languages are represented for subsets of the documents.}

We induced parts of speech by conversion from the gold PTB trees for the NAIST-NTT data, 
and for the remaining data, by automatic tagging with an averaged structured perceptron model (Rungsted\footnote{\url{https://github.com/coastalcph/rungsted}}) 
trained on the English Universal Dependencies v1.2 treebank \citep{nivre-15}.\footnote{\url{http://hdl.handle.net/11234/1-1548}}

\subsection{Comparing Domains}

A natural question to ask about lexical semantic annotations is whether we observe strong differences between domains.
For example, which kinds of multiword expressions and which kinds of supersenses 
occur more often in some domains than in others?
In this section, we report our observations but do not make any strong claims about their generality, for the following reasons:
 the samples are not necessarily representative of their domains overall, 
and, in fact, may have been sampled in a biased way (e.g., the Lowlands sample was limited to tweets containing a URL, 
and as a result, most of these tweets are headlines and advertisements).
Furthermore, the annotation procedures differed by subcorpus, likely biasing the results.


\begin{figure}
\includegraphics[width=\columnwidth]{figs/mwes-pos.pdf}
\caption{Counts of MWE occurrences, grouped by the POS of the first word in the MWE. 
Blue bars represent POSes that tend to start nominal MWEs; red bars roughly capture verbal MWEs.}
\label{fig:mwes-pos}
\end{figure}

\paragraph{MWEs.} \Cref{fig:mwes-pos} summarizes MWEs in the seven subcorpora with respect to syntactic status. 
Colors represent the POS tag of the first word in the MWE. 
Starting with proper nouns, the blue bars indicate POS tags that tend to begin nominal MWEs (noun, adjective, determiner, etc.).
Red bar POS tags are characteristic of verbal MWEs. The remaining bars are prepositional (dark green) 
and other miscellaneous tags, which collectively comprise no more than 10\% of the MWEs in each subcorpus.

It is worth noting that in this plot, subcorpora within the same domain 
are sometimes more divergent than subcorpora in different domains. 
Lowlands stands out as having a large share of proper noun MWEs---presumably due to the headline-oriented 
nature of the sample. STREUSLE has the smallest proportion of nominal MWEs, 
perhaps owing to the way it was annotated: 
initial rounds of STREUSLE annotation targeted MWEs only, with noun and verb supersenses 
added only later; whereas in the other data sets, MWE and supersense annotation were performed jointly, 
so annotator attention may have been focused on nominal and verbal expressions 
rather than other MWEs.

\begin{figure*}
\includegraphics[width=\textwidth]{figs/supersense-domains.pdf}
\caption{Supersense rate differences by domains, compared to reviews data set. 
Circle area proportional to the supersense's \emph{total} frequency across all domains.
Noun supersenses on the left, verb supersenses on the right. 
Each domain's rate is microaveraged across its subcorpora; 
thus, larger subcorpora weigh more heavily than smaller subcorpora in the same domain.}
\label{fig:supersense-domains}
\end{figure*}

\paragraph{Supersenses.} In the spirit of \citet{schneider-12}, we performed an analysis to see 
which supersenses were more characteristic of some domains than others. 
\Cref{fig:supersense-domains} plots the relative frequency (out of all supersense-labeled units) 
of each supersense in each of the three domains. We use the \dataset{Reviews} domain as base frequency: 
relative to that, 
the x-axis is the supersense's occurrence rate in the \dataset{Tweets} domain, 
and the y-axis represents the rate for the \dataset{TED} talks.

These plots show some clear outliers: among nouns (left plot), 
\sst{n.group} and \sst{n.food} are overrepresented in \dataset{Reviews} 
relative to the other domains---unsurprising because restaurants and other businesses are prominent in this subcorpus.
On the other hand, \sst{n.person} is underrepresented in \dataset{Reviews}. 
\sst{n.time} and \sst{n.communication} are more popular in the \dataset{Tweets} domain than the others.
Among verbs (right plot), \sst{v.stative} is underrepresented, apparently due to the relative rarity of 
the copula (which often can be safely omitted in headlines and other telegraphic messages 
without obscuring the meaning).


\begin{figure*}[t] %\small
% {"34": ["price", "POSSESSION"], "36": ["means", "cognition"], "26": ["was", "stative"], "29": ["budge", "change"]}
\begin{framed}\small
\emph{MWE Precision:} The proportion of predicted links whose words 
both belong to the same expression in the gold standard. \\
\emph{MWE Recall:} Same as precision, but swapping the predicted and gold annotations. %\\
%\emph{Strength Averaging:} A weak link is treated as intermediate between a strong link and no link at all: 
%precision, recall, and $F_1$ computed on strong links only are averaged 
%with the respective calculations computed on all links without regard to strength.
\end{framed}\centering
\begin{tikzpicture}[baseline={($(current bounding box.center)+(0,-0.5ex)$)}, node distance=0cm, auto,]
 \node[inner sep=0cm]                                  (n1) {\tagtss{The}{O}{}{O}{}};
 \node[inner sep=0cm,above right=0pt of n1.south east] (n2) {\tagtss{staff}{O}{\textsc{n.group}}{\color{orange}B}{\color{orange}n.group}};
 \node[inner sep=0cm,above right=0pt of n2.south east] (n3) {\tagtss{leaves}{\color{red}B}{\color{red}v.cognition}{\color{orange}I}{}};
 \node[inner sep=0cm,above right=0pt of n3.south east] (n4) {\tagtss{a}{\color{blue}b}{}{\color{orange}I}{}};
 \node[inner sep=0cm,above right=0pt of n4.south east] (n5) {\tagtss{lot}{\color{blue}i}{}{\color{orange}I}{}};
 \node[inner sep=0cm,above right=0pt of n5.south east] (n6) {\tagtss{to}{\color{red}I}{}{\color{orange}I}{}};
%\path[-,thick,dotted, bend left=30] (n2.north) edge (n3.north); 
\path[-,thick,red, bend left=20] (n3.north) edge (n6.north);
 \path[-,thick,blue, bend left=30] (n4.north) edge (n5.north);
\path[-,thick,orange, bend right=20] (n2.south) edge (n3.south);
\path[-,thick,orange, bend right=20] (n3.south) edge (n4.south);
\path[-,thick,orange, bend right=20] (n4.south) edge (n5.south);
\path[-,thick,orange, bend right=20] (n5.south) edge (n6.south);
 \node[inner sep=0cm,above right=0pt of n6.south east] (n7) {\tagtss{be}{\color{red}I}{}{\color{orange}I}{}};
\path[-,thick,orange, bend right=20] (n6.south) edge (n7.south); 
 \node[inner sep=0cm,above right=0pt of n7.south east] (n8) {\tagtss{desired}{\color{red}I}{}{O}{v.emotion}};
\path[-,thick,red, bend left=30] (n6.north) edge (n7.north);
\path[-,thick,red, bend left=30] (n7.north) edge (n8.north);
 \node[inner sep=0cm,above right=0pt of n8.south east] (n9) {\tagtss{.}{O}{}{O}{}};
\end{tikzpicture}
\caption{A \textsc{Reviews} sentence with MWE and supersense analyses: gold above and hypothetical prediction below. 
MWE precision of the bottom annotation relative to the top one is $2/5$. %with weak links removed 
%and $2/6$ with weak links strengthened to strong links
(Note that a link between words $w_1$ and $w_2$ is ``matched'' 
if, in the other annotation, there is a path between $w_1$ and $w_2$.) 
The MWE recall value is $3/4$. 
Supersense precision and recall are both $1/2$.
Combined precision/recall scores add the respective subscores' numerators and denominators:
thus, combined precision is $\tfrac{2+1}{5+2} = 3/7$, 
and combined recall is $\tfrac{3+1}{4+2} = 2/3$.
Combined $F_1$ is their harmonic mean, i.e.~$12/23$.
%Overall $F_1$ is computed as the average of two $F_1$-scores, i.e.~$\tfrac{1}{3} \cdot \tfrac{1}{1}/(\tfrac{1}{3} + \tfrac{1}{1}) + \tfrac{2}{6}\cdot \tfrac{3}{3}/(\tfrac{2}{6}+\tfrac{3}{3}) = 0.50$.
}
\label{fig:linkmeasure}
\end{figure*}


\section{Evaluation}\label{sec:eval}

\paragraph{Submission conditions.}
% \nss{open, closed condition. if a team submits systems to the closed condition and fewer than 3 systems to the open condition, 
% they will be encouraged to submit the closed condition results to the open condition as well.}
%The main goal of the task was to find creative new ways to improve the performance of lexical semantic analysis on various domains. 
%In keeping with conventions of past shared tasks at SemEval and other venues, 
We invited submissions in multiple data conditions. %These conditions were judged separately, and teams were welcome to submit systems under more than one.
The \textbf{open} condition encouraged participants to make 
wide use of any and all available resources, including for 
distant or direct supervision. 
A \textbf{closed} condition encouraged controlled comparisons of algorithms 
by limiting their training to specific resources distributed for the task. 
Lastly, we allowed for a \textbf{semi-supervised closed} condition, in which 
use of a specific large unlabeled corpus---the Yelp Academic Dataset\footnote{\url{https://www.yelp.com/academic_dataset}}---was permitted.
Teams were permitted to submit no more than one run per condition.
Only one team submitted a system in the semi-supervised closed condition.

%a \textbf{supervised} closed condition will have access only to resources distributed for the task, 
%and a \textbf{semi-supervised} closed condition will further allow use of a specific unannotated corpus.\footnote{%
%We propose the Yelp Academic Dataset (\url{https://www.yelp.com/academic_dataset}), 
%which is freely available for research use, as the unlabeled corpus for the semi-supervised closed condition.
All conditions had access to: 1)~the annotated data we provided; 
2)~Brown clusterings \citep{brown-92} computed from large corpora of tweets and web reviews;\footnote{I.e., TweetNLP clusters (\url{http://www.cs.cmu.edu/~ark/TweetNLP/}) and 
the Yelp Academic Dataset clusters used in AMALGrAM (\url{http://www.cs.cmu.edu/~ark/LexSem/}).} 
and 3)~the English WordNet lexicon. 
The input at test time included POS tags. 

No sentence-level metadata was provided in the input at test time: 
test set sentence IDs were obscured to hide the source domain, and the order of sentences was randomized 
to remove document structure.
The training data, however, marked the domain from which the sentence was drawn 
(\textsc{Reviews} or \textsc{Tweets}); systems were free to make use of this information, 
so long as it was not required as part of the input at test time.

% \nss{a few possibilities for conditions---
% \begin{description}
% \item[closed condition] provided training data; we'll allow WordNet and provide (auto?)~POS, Brown clusters as well? 
% no other resources allowed \dirk{yes: this will give people an ``out-of-the-box'' task opportunity}
% \item[semi-open condition] we provide a large unlabeled corpus, in addition to closed condition resources \dirk{would nix that: people should choose their own unlabeled data}
% \item[unsupervised condition] no token-level MWE or SST labels may be used \dirk{maybe for future iterations of the task?}
% \item[open condition] any data/resources (except the CMWE corpus, if that includes the test set) may be used
% \end{description}
% I feel like there are pluses and minuses to all 4 of these conditions. maybe we should just mention the possibilities 
% and survey teams to ensure we only evaluate conditions that generate sufficient interest}

\paragraph{Scoring.}
We provided an evaluation script to allow participants to check the format of system output and to compute all official scores.

The \textbf{MWE} measure looks at precision, recall, and $F_1$ 
of the identified MWEs. Tokens not involved in a predicted or gold MWE
do not factor into this measure. 
To award partial credit for partial overlap between a predicted MWE 
and a gold MWE, these scores are computed based on \emph{links} between consecutive tokens in an expression \citep{schneider-14}. 
The tokens must appear in order but do not need to be adjacent.
The precision is the proportion of predicted links whose words 
both belong to the same expression in the gold standard. 
Recall is the same as precision, but swapping the predicted and gold annotations.\longversion{\footnote{This computation on the basis of links 
is a slight simplification of the MUC coreference measure \citep{vilain-95}.}}
\Cref{fig:linkmeasure} defines this measure in detail and illustrates the calculations for an example.
%\Cref{fig:linkmeasure} illustrates the links implied by two different annotations 
%of a sentence. 

%  A weak link is treated as intermediate between a strong link and no link at all: 
%  precision, recall, and $F_1$ computed on strong links only are averaged 
%  with the respective calculations computed on all links without regard to strength. 
%  \Cref{fig:linkmeasure} illustrates these calculations with an example.

To isolate the \textbf{supersense} classification performance, 
we compute precision, recall, and $F_1$ of the supersense-labeled word tokens.
The numerator of both precision and recall is the number of tokens 
labeled with the correct supersense.
(This interacts slightly with MWE identification, however, 
as supersenses are only marked on the first token of MWEs. 
We do not mark supersenses on all words of the MWE to avoid 
giving MWEs a disproportionate influence on the supersense score.)
% \longversion{
% Because the supersense of a strong MWE is marked on its \emph{first} word, 
% this will have the effect of penalizing errors on the left boundary of an MWE, 
% but it is less sensitive to the MWE analysis than the link-based measure.}

Finally, \textbf{combined} precision, recall, and $F_1$ 
aggregate the MWE and supersense subscores.
The combined precision ratio is computed from the MWE and supersense precision ratios 
by adding their numerators and denominators, and likewise for combined recall 
(see the example in \cref{fig:linkmeasure}).

Within each domain, scores are computed as microaverages.
The official tri-domain scores reported here are domain macroaverages:  
per-domain measures are aggregated with the three domains weighted equally. 
The main score, tri-domain combined $F_1$, is the arithmetic mean of the three per-domain combined $F_1$ scores.
(Some system papers report domain microaverages, which give less influence to the \dataset{TED} domain 
because it is the smallest of the domains in the test set.)




\section{Entries and Results}\label{sec:results}

Six teams\footnote{None of the teams included any DiMSUM organizers.} participated in the task, 
submitting a total of nine unique system entries prior to the deadline. 
We give an overview of these systems and analyze their performance.

\subsection{Synopsis of approaches}

%P332 = \sys{S106} (UFRGS\&LIF, Nathan):
From the \textbf{UFRGS\&LIF} team \citep{dimsum-16-ufrgs},
\sys{S106} detects MWEs by heuristic pattern-matching against sequences in the training data, 
and tags supersenses based on the first WordNet sense of each word.

%P359  (UTU, Dirk):
From the \textbf{UTU} team \citep{dimsum-16-utu},
\sys{S211}, \sys{S254}, and \sys{S255} 
match word sequences against a variety of resources
and then choose a supersense with an ensemble of classifiers. 
The method performs reasonably well for supersenses, but is weak at detecting MWEs.


%P367 =  (UW-CSE, Dirk):
The \textbf{UW-CSE} team \citep{dimsum-16-uw} 
experimented with a sequence CRF as well as a double-chained CRF, 
with separate chains for MWE tags and supersenses, and some parameters shared between them.
The closed-condition and open-condition feature sets were drawn from AMALGrAM \citep{schneider-15}.
Of the official submissions, \sys{S248} used a single-chain CRF and \sys{S249} a double-chained CRF.
A full comparison demonstrates that the double-chained CRF performs best on the combined measure 
in both the closed and open conditions. 

%P433 =  (ICL-HD, Nathan):
From the \textbf{ICL-HD} team \citep{dimsum-16-icl},
\sys{S214} uses the AMALGrAM sequence tagger \citep{schneider-15} 
with an augmented feature set that leverages word embeddings 
and a knowledge base. The word embedding features, the knowledge base--derived 
features, and their union all improve over the condition with no new features, 
with respect to both MWE performance and supersense performance. 
The best results for the combined measure are obtained with the 
word embedding features (but not the knowledge base features). 
The word embeddings are shown to be somewhat complementary 
to AMALGrAM's Brown cluster features: ablating either reduces performance.

\begin{table}\small\centering
\begin{tabular}{rllrl}
 \#   & \textbf{System} &           \textbf{Team} &  \textbf{Score}  & \textbf{Resources} \\
\midrule
    1 &   \sys{S214} &         ICL-HD &  57.77 &   ++ \\
      &   \sys{S249} &         UW-CSE &  57.71 &   ++ \\
      &   \sys{S248} &         UW-CSE &  57.10 &      \\
    2 &   \sys{S106} &     UFRGS\&LIF &  50.27 &      \\
    3 &   \sys{S227} &  VectorWeavers &  49.94 &   ++ \\
    4 &   \sys{S255} &            UTU &  47.13 &   ++ \\
    5 &   \sys{S211} &            UTU &  46.17 &    + \\
      &   \sys{S254} &            UTU &  45.79 &      \\
    6 &   \sys{S108} &         WHUNlp &  25.71 &      \\
\end{tabular}

\caption{Main results on the test set. Scores are tri-domain combined $F_1$ percentages. 
Resource conditions are described in \cref{sec:overall}.}

\label{tbl:main-results}
	
\end{table}


%P347 =  (WHUNlp, AJ): 
From the \textbf{WHUNlp} team (Tang Xin and Li Fei, Wuhan University),
\sys{S108} uses a pipeline where a sequence CRF first identifies MWEs, 
and a maximum entropy classifier then predicts a supersense independently 
for each lexical expression. Each of these models has a small number of feature templates 
recording words and POS tags.

%P531 = S227 (team name: VectorWeavers) (Marine):  
From the \textbf{VectorWeavers} team \citep{dimsum-16-vector},
\sys{S227} relies on neural network classifiers to detect MWE boundaries and label supersenses, 
using features based on word embeddings and syntactic parses. 
Results show that syntax helps identify MWE boundaries accurately, and that 
simple incremental composition functions can help construct useful MWE representations.


\subsection{Overall results}\label{sec:overall}
The main results appear in \cref{tbl:main-results}. 
The first column of \cref{tbl:main-results} gives the ranking of the systems. 
Several systems may share a rank if they do not produce significantly different predictions, as detailed below. 
The score is the combined supersense and MWE measure, macroaveraged over the three test set domains 
as described above. 
The final column indicates the resource condition:
systems entered in the open condition (all resources allowed) are designated ``++''; 
``+'' indicates the more restricted semi-supervised closed condition,  
while the remaining systems are in the closed condition (most restrictive). 
Details of the resource conditions and scoring appear in \cref{sec:eval}.


\begin{table}[t]\small\centering
\begin{tabular}{lr<{\hspace{10pt}}rr<{\hspace{7pt}}H}
\textbf{Submission} &  \multicolumn{1}{c}{\textbf{\dataset{Reviews}}} &   \multicolumn{1}{c}{\textbf{\dataset{TED}}} 
   &  \multicolumn{1}{c}{\textbf{\dataset{Tweets}}} &  \multicolumn{1}{H}{\textbf{$\mu$avg}} \\
%\midrule
\cmidrule{1-4}
\multicolumn{5}{l}{\textbf{Multiword expressions}} \\

\sys{S106}       &   49.57 &  56.76 &   51.16 &      51.48 \\
\sys{S108}       &   26.39 &  33.44 &   34.18 &      30.98 \\
\sys{S211} +     &    9.07 &  18.28 &   15.76 &      13.46 \\
\sys{S214} ++    &   53.37 &  \textbf{57.14} &   59.49 &      56.66 \\
\sys{S227} ++    &   36.18 &  41.76 &   39.32 &      38.49 \\
\sys{S248}       &   53.96 &  52.35 &   54.48 &      53.93 \\
\sys{S249} ++    &   \textbf{54.80} &  53.48 &   \textbf{61.09} &      \textbf{57.24} \\
\sys{S254}       &    7.05 &  16.30 &    6.34 &       8.20 \\
\sys{S255} ++    &    8.68 &  20.11 &   15.50 &      13.48 \\
{} & \\[-1.4ex]

\multicolumn{5}{l}{\textbf{Supersenses}} \\

\sys{S106}       &           50.93  &          49.61  &         49.20  &      49.98 \\
\sys{S108}       &           25.82  &          24.68  &         24.63  &      25.14 \\
\sys{S211} +     &           52.00  &          51.40  &         49.95  &      51.11 \\
\sys{S214} ++    &   \textbf{57.66} &  \textbf{60.06} &         55.99  &      57.55 \\
\sys{S227} ++    &           51.36  &          52.00  &         51.70  &      51.62 \\
\sys{S248}       &           57.19  &          59.11  &         56.82  &      57.47 \\
\sys{S249} ++    &           57.00  &          59.17  & \textbf{57.46} &  \textbf{57.64} \\
\sys{S254}       &           52.68  &          51.44  &         49.66  &      51.29 \\
\sys{S255} ++    &           51.98  &          53.28  &         51.11  &      51.93 \\
{} & \\[-1.4ex]

\multicolumn{5}{l}{\textbf{Combined score}} \\

\sys{S106}       &   50.71 &  50.57 &   49.54 &      50.22 \\
\sys{S108}       &   25.86 &  25.39 &   25.87 &      25.76 \\
\sys{S211} +     &   46.19 &  47.90 &   44.42 &      45.86 \\
\sys{S214} ++    &   \textbf{56.98} &  \textbf{59.71} &   56.63 &      57.41 \\
\sys{S227} ++    &   49.25 &  50.82 &   49.74 &      49.77 \\
\sys{S248}       &   56.66 &  58.26 &   56.38 &      56.88 \\
\sys{S249} ++    &   56.61 &  58.33 &   \textbf{58.18} &      \textbf{57.57} \\
\sys{S254}       &   46.57 &  47.82 &   42.99 &      45.47 \\
\sys{S255} ++    &   46.15 &  49.81 &   45.44 &      46.64 \\


\end{tabular}

\caption{Per-domain evaluation results. Figures are $F_1$ percentages.
The best value in each section and column is in bold.
Refer to \cref{tbl:main-results} for the identities of the systems.}	
\label{tbl:per-domain-results}
\end{table}


\paragraph{Ranking and significance.}
The overall best scoring system, with a combined measure of 57.77\%, is \sys{S214}. 
The competition, however, is close: \sys{S249} scored 57.71\%, and \sys{S248} obtained a combined score of 57.10\%.  
To check whether the predictions of the systems are significantly different from each other, 
we ran McNemar's test, a paired test that operates directly on the predicted system output. 
A consequence of this is that we do not directly test whether the computed \emph{scores} 
are significantly different from each other, only whether the \emph{predictions} are. 

According to McNemar's test, the predictions of the highest-ranking and the next-highest-ranking system 
are not significantly different at $p < .05$. The third highest ranking system performs significantly worse 
than the top system, but is \emph{not} significantly different from the second-place system. 
We therefore decided to rank all three systems together. 
In general, adjacent entries in the sorted scoring table are ranked together 
if the difference between them is not statistically significant according to the test. 

\begin{figure}
	\includegraphics[width=8.5cm]{figs/supersense_predictions.pdf}
	\caption{Number of systems predicting the correct supersense (for tokens where there is a gold supersense). 
%\aj{Slight edit: the graph is not technically a histogram.}. \aj{I did try to produce a version with each bar split into domains. It turns out that there are practically no domain differences. Only thing I found was that Trustpilot supersenses have a slightly higher proportion of \emph{both} the easy and the hard cases. }
}
	\label{fig:supersense-predictions}
\end{figure}


\paragraph{Drilling down.}
\Cref{tbl:per-domain-results} offers a more detailed breakdown by domain and 
subscore (MWEs vs.~supersenses vs.~combined). 
The best scores are about 57\% for both MWEs and supersenses. 
Systems \sys{S214} and \sys{S249} are the clear winners: 
the former is better in the surprise \dataset{TED} domain---particularly TED MWEs (by nearly 4~points).
The latter is slightly better in \dataset{Tweets}, 
and the systems are quite close in \dataset{Reviews} (the domain with the most training data).

\sys{S214} and \sys{S249} were in the open condition, taking advantage of additional resources. 
The best system in the closed condition is \sys{S248}, which is very similar to \sys{S249}---and 
recall that its predictions, overall, are not statistically worse.  
\Cref{tbl:per-domain-results} reveals one striking difference, however: 
in MWE scores for \dataset{Tweets}, \sys{S249} bests \sys{S248} by nearly 7 points.

When scores in the 3~domains are compared for each system, there is surprisingly little difference overall.
We expected that the \dataset{TED} domain would be most difficult because it is not represented 
in the training data, but the scores in \cref{tbl:per-domain-results} give no clear indication that this is the case. 
Perhaps systems escaped domain bias because the training data included two highly divergent genres; 
or perhaps other aspects of the data sets (e.g., topic) matter more for this task than 
differences in genre.  









\subsection{Easy and hard decisions}

Overall, the results clearly show that the joint supersense and MWE tagging task is not yet resolved. 
Given the wide range of participating systems and previous work, it is reasonable to assume that 
the task itself is not easy. On the other hand, it is not \emph{uniformly} hard. 
In fact, some decisions are relatively easy, in the sense that most or all systems get them right; 
whereas others are hard, in that none or very few systems produce the correct answer. 
\Cref{fig:supersense-predictions} explores this for the supersense-tagging subtask. 
The tallest bars are near the left and right sides of the graph, representing the hard and easy instances, respectively. 
Hard instances account for about 25\% of instances where the gold data has a supersense, 
which also puts an upper bound on any system combination. 
Even an oracle system allowed to choose the best prediction for each instance 
from among all the systems would still not push the accuracy above 75\%.

\begin{figure}[t]
	\includegraphics[width=8cm]{figs/proportion_of_easy_and_hard_supersenses.pdf}
	\caption{Easy and hard supersense decisions. Shown in blue in the left side of the plot is the proportion of instances of the given supersense type where at most one system gave the \emph{wrong} answer. On the right side in red is the corresponding figure where at most one system gave the \emph{right} answer. Supersenses are sorted by corpus frequency.}
	\label{fig:easy-and-hard-supersenses}
\end{figure}

The distribution of easy and hard instances varies a lot between labels, though. 
As shown for supersenses in \cref{fig:easy-and-hard-supersenses}, 
individual labels range from the fairly easy (e.g.\ \textsc{v.stative} and \textsc{v.communication}) 
to the more difficult (e.g.\ \textsc{n.attribute} and \textsc{v.contact}). 
The most common supersense, \textsc{v.stative}, is easy because it has few distinct lexical forms 
(the ten most common lemmas make up more than 77\% of the instances).
%, and so the $P(\textit{supersense} \mid \textit{lemma})$ mapping has low entropy.\nss{backwards?}
Examples of \textsc{v.stative} lemmas include \emph{be}, \emph{have}, \emph{use}, and \emph{get}. 

Supersenses may be difficult for more than one reason. 
For instance, \textsc{v.contact}---e.g.\ \emph{deliver}, \emph{receive}, and \emph{take}---has more distinct forms 
than \textsc{v.stative} and also a more complex mapping between lemmas and supersenses. 
In contrast, person names, job titles, etc.\ that should be tagged as \textsc{n.person} are rarely ambiguous
with respect to supersense. The main challenge in that case is that the category is open-ended 
and not in general evident from syntactic structure.

\subsection{System correlation}

Finally, we examine whether the submitted approaches capture different aspects of the task. 
I.e., could we produce a better system by combining the individual systems? 
We cannot estimate this from the results tables, since, combinatorially, 
there are many ways to obtain a given score. 
However, we can estimate it from the prediction overlap between systems.
The $N \times N$ labeled matrix in \cref{fig:system-clusters} 
shows how the $N$ systems relate to each other. 
Each cell compares the predictions of two systems $a$ and $b$ in the joint supersense and MWE task. 
The value of a cell $T_{a,b}$ is the number of correct predictions made by $a$ 
that were not correctly predicted by $b$. This is an asymmetric measure of predictive similarity. 
A single low number indicates one out of two things: either the systems are similar, or $a$ is better than $b$. 
When the sum $T_{a,b} + T_{b,a}$ is small, the two systems make similar predictions. 

Clustering the systems in \cref{fig:system-clusters} (shown on the left side of the plot) 
results in groups that correspond to the ranking in \cref{tbl:main-results}. 
Inside the cluster of systems ranked at 1, the asymmetric predictive advantage ranges between 267 and 469. 
Lower-ranked systems all have a smaller predictive advantage with respect to the top-ranked systems. 
The best combination system would thus likely be between two of the rank-1 systems. 
However, the gains are small, and overall the systems seem to extract the same knowledge, 
or subsets of the same knowledge, out of the training data. 
% Additionally, while there is a small but statistically significant difference between the best system of the open condition and the best system of the closed condition, 






\begin{figure}
	\includegraphics[width=\columnwidth]{figs/system_pairwise_clusters_cropped.pdf}
	\caption{System clusters. Each cell compares the predictions of two systems $i$ and $j$ with respect to a gold standard. The value in the $i$,$j$-th cell is the number of predictions that $i$ got right but $j$ did not.}
	\label{fig:system-clusters}
\end{figure}













\section{Conclusion}
This task featured a broad-coverage lexical semantic analysis task that combines MWE identification and supersense tagging. 
The semantic tagset strikes a balance between the extremely difficult fine-grained distinctions in classical WSD, 
and the restrictiveness of the NER task. To guard against domain bias, we provided training data 
from two different genres, namely online reviews and tweets, as well as a test-only data set with TED talk transcripts. 
The training and test data sets are publicly available at \url{https://github.com/dimsum16/dimsum-data}.

The best scoring systems obtained 57.7\% $F_1$ on a composite measure over the two subtasks of MWE and supersense tagging, 
averaged over the three test domains. This level of performance suggests that the task is not yet resolved. 
Furthermore, our error analysis suggests that the submitted systems  
arrived at similar generalizations from the training data. 
Substantially improving performance would thus seem to require novel approaches. 

\section*{Acknowledgments}

We are grateful to Sam Gibbon for the lexical semantic annotation,
which was generously supported by Carlsberg infrastructure grant No.~CF14-0694.




\bibliographystyle{style/aclnat}
% you bib file should really go here
\setlength{\bibsep}{10pt}
{\fontsize{10}{12.25}\selectfont
\bibliography{dimsum}}



\end{document}
